{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9177a9b2-deef-451d-8bc9-d2377f12c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required install libraries:\n",
    "#        datasets\n",
    "#        transformers\n",
    "#        PyTorch\n",
    "#        \n",
    "import os\n",
    "\n",
    "# Defining constants\n",
    "MODEL_SAVE_PATH = \"./trained_model\"\n",
    "DATA_DIR = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eef21d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 748 documents from ../data\n"
     ]
    }
   ],
   "source": [
    "# Loads data from data_XXXX.txt and label_XXXX.txt files\n",
    "def load_local_dataset(data_dir):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    # Recursively search for files in the dataset directory and subdirectories\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for filename in files:\n",
    "            if filename.startswith(\"data_\") and filename.endswith(\".txt\"):\n",
    "                data_filepath = os.path.join(root, filename)\n",
    "                label_filepath = data_filepath.replace(\"data_\", \"label_\")  # Assuming matching filenames for data and labels\n",
    "                \n",
    "                # Read the text\n",
    "                with open(data_filepath, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read().strip()\n",
    "                    texts.append(text)\n",
    "                \n",
    "                # Read the corresponding label\n",
    "                with open(label_filepath, 'r', encoding='utf-8') as f:\n",
    "                    label = f.read().strip()\n",
    "                    labels.append(label)\n",
    "    \n",
    "    # Return a list of dicts with 'text' and 'label' keys\n",
    "    return [{\"text\": t, \"label\": l} for t, l in zip(texts, labels)]\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_local_dataset(DATA_DIR)\n",
    "print(f\"Loaded {len(dataset)} documents from {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e3ed9b-c034-4a0a-b629-b98debbbc053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert loaded data to Hugging Face Dataset format\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset into a Hugging Face Dataset object\n",
    "dataset = Dataset.from_dict({\"text\": [item['text'] for item in dataset],\n",
    "                             \"label\": [item['label'] for item in dataset]})\n",
    "\n",
    "# Optionally split into train/validation/test sets\n",
    "dataset = dataset.train_test_split(test_size=0.2)  # 80/20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dab380-43c7-47c3-9e79-93eac9b053a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Fine-Tuning\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoModel\n",
    "\n",
    "# Load the tokenizer\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Adjust num_labels to your task\n",
    "\n",
    "def str2int(str):\n",
    "    if str == \"nao-protesto\":\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized[\"label\"] = [str2int(label) for label in examples[\"label\"]]\n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39626293-423d-446e-8b99-b81d34e76df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964e041-b4a0-4efa-a234-5c22007474fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both the model and tokenizer\n",
    "trainer.save_model(MODEL_SAVE_PATH)  # Saves the model\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)  # Saves the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f3062-e348-4cd4-b23b-20f8a38bedaa",
   "metadata": {},
   "source": [
    "# Now let's run inference on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836df5d-8221-410f-b3c6-f149eaa43799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_SAVE_PATH)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) \n",
    "vocabDict = tokenizer.get_vocab()\n",
    "\n",
    "def getClass(pred):\n",
    "    if pred == 0:\n",
    "        return 'nao-protesto'\n",
    "    return 'protesto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeefbe22-3b03-4220-be65-85f883d5fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleText = \"Um homem, suspeito de ter aplicado um golpe em um idoso de Paranaguá, teve o celular e dispositivos eletrônicos apreendidos nesta quinta-feira (17). O crime aconteceu em abril deste ano, quando o suspeito fingiu que era o filho da vítima e o convenceu a depositar todo o salário na conta bancária do criminoso. De acordo com as investigações do Gaeco de Paranaguá, o idoso teve prejuízos financeiros e abalos psicológicos. A apreensão dos itens do suspeito foi feita pelo Ministério Público no município de Aparecida, em Goiás.De acordo com as investigações, o suspeito criava perfis falsos no WhatsApp para pedir dinheiro. O delegado que atua no Gaeco, Fernando de Carvalho Santana explica que casos de estelionato como esse são comuns e por isso as pessoas devem prestar mais atenção.A Polícia Civil do Paraná tem uma cartilha com dicas para orientar a população sobre esses golpes aplicados por meio de mensagens, para que as pessoas aprendam a reconhecer e a evitar a ação de estelionatários. Acesse aqui a cartilha da Polícia Civil.Reportagem: Brenda Niewiorowski\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(exampleText, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972e662-2112-45e4-aca2-79002df9e715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Run inference with the trained model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the model's logits and apply softmax to get probabilities\n",
    "logits = outputs.logits\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "\n",
    "# Output the result\n",
    "print(f\"Predicted class: {getClass(predicted_class)}\")\n",
    "print(f\"Probabilities: {probabilities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1c7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
