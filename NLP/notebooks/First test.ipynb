{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9177a9b2-deef-451d-8bc9-d2377f12c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required install libraries:\n",
    "#        datasets\n",
    "#        transformers\n",
    "#        PyTorch\n",
    "#        \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "383b2191-d991-430d-a464-9141d2ee69f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data from data_XXXX.txt and label_XXXX.txt files\n",
    "def load_local_dataset(data_dir):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over the files in your dataset directory\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.startswith(\"data_\") and filename.endswith(\".txt\"):\n",
    "            data_filepath = os.path.join(data_dir, filename)\n",
    "            label_filepath = data_filepath.replace(\"data_\", \"label_\")  # Assuming matching filenames for data and labels\n",
    "            \n",
    "            # Read the text\n",
    "            with open(data_filepath, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "                texts.append(text)\n",
    "            \n",
    "            # Read the corresponding label\n",
    "            with open(label_filepath, 'r', encoding='utf-8') as f:\n",
    "                label = f.read().strip() \n",
    "                labels.append(label)\n",
    "    \n",
    "    # Return a list of dicts with 'text' and 'label' keys\n",
    "    return [{\"text\": t, \"label\": l} for t, l in zip(texts, labels)]\n",
    "\n",
    "# Define the path to your dataset folder\n",
    "data_dir = \"./training_data\"\n",
    "dataset = load_local_dataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eef21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data from data_XXXX.txt and label_XXXX.txt files\n",
    "def load_local_dataset(data_dir):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over the files in your dataset directory\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.startswith(\"data_\") and filename.endswith(\".txt\"):\n",
    "            data_filepath = os.path.join(data_dir, filename)\n",
    "            label_filepath = data_filepath.replace(\"data_\", \"label_\")  # Assuming matching filenames for data and labels\n",
    "            \n",
    "            # Read the text\n",
    "            with open(data_filepath, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "                texts.append(text)\n",
    "            \n",
    "            # Read the corresponding label\n",
    "            with open(label_filepath, 'r', encoding='utf-8') as f:\n",
    "                label = f.read().strip() \n",
    "                labels.append(label)\n",
    "    \n",
    "    # Return a list of dicts with 'text' and 'label' keys\n",
    "    return [{\"text\": t, \"label\": l} for t, l in zip(texts, labels)]\n",
    "\n",
    "# Define the path to your dataset folder\n",
    "data_dir = \"./training_data\"\n",
    "dataset = load_local_dataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66e3ed9b-c034-4a0a-b629-b98debbbc053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert loaded data to Hugging Face Dataset format\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset into a Hugging Face Dataset object\n",
    "dataset = Dataset.from_dict({\"text\": [item['text'] for item in dataset],\n",
    "                             \"label\": [item['label'] for item in dataset]})\n",
    "\n",
    "# Optionally split into train/validation/test sets\n",
    "dataset = dataset.train_test_split(test_size=0.2)  # 80/20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9dab380-43c7-47c3-9e79-93eac9b053a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db79ac7a0ea41df8679fe3c30184e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c6cc18be834ad38855397451dab148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization and Fine-Tuning\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoModel\n",
    "\n",
    "# Load the tokenizer\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Adjust num_labels to your task\n",
    "\n",
    "def str2int(str):\n",
    "    if str == \"nao-protesto\":\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized[\"label\"] = [str2int(label) for label in examples[\"label\"]]\n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39626293-423d-446e-8b99-b81d34e76df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc5aa0ee0b244e7bad78d38d5f120bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34171d68468c4cc9a2219fad83f76bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7274514436721802, 'eval_runtime': 0.0571, 'eval_samples_per_second': 17.5, 'eval_steps_per_second': 17.5, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7fa95b21354194b9f52a68be43140f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6123903393745422, 'eval_runtime': 0.0542, 'eval_samples_per_second': 18.465, 'eval_steps_per_second': 18.465, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ed26e04aae4772a82fbe1145c5cc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5765517354011536, 'eval_runtime': 0.1145, 'eval_samples_per_second': 8.732, 'eval_steps_per_second': 8.732, 'epoch': 3.0}\n",
      "{'train_runtime': 12.3468, 'train_samples_per_second': 0.972, 'train_steps_per_second': 0.243, 'train_loss': 0.633892297744751, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.633892297744751, metrics={'train_runtime': 12.3468, 'train_samples_per_second': 0.972, 'train_steps_per_second': 0.243, 'total_flos': 3157332664320.0, 'train_loss': 0.633892297744751, 'epoch': 3.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b964e041-b4a0-4efa-a234-5c22007474fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_model\\\\tokenizer_config.json',\n",
       " './trained_model\\\\special_tokens_map.json',\n",
       " './trained_model\\\\vocab.txt',\n",
       " './trained_model\\\\added_tokens.json',\n",
       " './trained_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save both the model and tokenizer\n",
    "trainer.save_model(\"./trained_model\")  # Saves the model\n",
    "tokenizer.save_pretrained(\"./trained_model\")  # Saves the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f3062-e348-4cd4-b23b-20f8a38bedaa",
   "metadata": {},
   "source": [
    "# Now let's run inference on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4836df5d-8221-410f-b3c6-f149eaa43799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your saved model\n",
    "model_path = \"./trained_model\"\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "def getClass(pred):\n",
    "    if pred == 0:\n",
    "        return 'nao-protesto'\n",
    "    return 'protesto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeefbe22-3b03-4220-be65-85f883d5fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleText = \"\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(exampleText, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9972e662-2112-45e4-aca2-79002df9e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: nao-protesto\n",
      "Probabilities: tensor([[0.5058, 0.4942]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Run inference with the trained model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the model's logits and apply softmax to get probabilities\n",
    "logits = outputs.logits\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "\n",
    "# Output the result\n",
    "print(f\"Predicted class: {getClass(predicted_class)}\")\n",
    "print(f\"Probabilities: {probabilities}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
