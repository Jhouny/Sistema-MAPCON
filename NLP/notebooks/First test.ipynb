{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9177a9b2-deef-451d-8bc9-d2377f12c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required install libraries:\n",
    "#        datasets\n",
    "#        transformers\n",
    "#        PyTorch\n",
    "#        \n",
    "import os, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eef21d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 733 documents from ../data\n"
     ]
    }
   ],
   "source": [
    "# Loads data from data_XXXX.txt and label_XXXX.txt files\n",
    "def load_local_dataset(data_dir):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over the files in your dataset directory\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.startswith(\"data_\") and filename.endswith(\".txt\"):\n",
    "            data_filepath = os.path.join(data_dir, filename)\n",
    "            label_filepath = data_filepath.replace(\"data_\", \"label_\")  # Assuming matching filenames for data and labels\n",
    "            \n",
    "            # Read the text\n",
    "            with open(data_filepath, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "                texts.append(text)\n",
    "            \n",
    "            # Read the corresponding label\n",
    "            with open(label_filepath, 'r', encoding='utf-8') as f:\n",
    "                label = f.read().strip() \n",
    "                labels.append(label)\n",
    "    \n",
    "    # Return a list of dicts with 'text' and 'label' keys\n",
    "    return [{\"text\": t, \"label\": l} for t, l in zip(texts, labels)]\n",
    "\n",
    "# Define the path to your dataset folder\n",
    "data_dir = \"../data\"\n",
    "dataset = load_local_dataset(data_dir)\n",
    "print(f\"Loaded {len(dataset)} documents from {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e3ed9b-c034-4a0a-b629-b98debbbc053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert loaded data to Hugging Face Dataset format\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset into a Hugging Face Dataset object\n",
    "dataset = Dataset.from_dict({\"text\": [item['text'] for item in dataset],\n",
    "                             \"label\": [item['label'] for item in dataset]})\n",
    "\n",
    "# Optionally split into train/validation/test sets\n",
    "dataset = dataset.train_test_split(test_size=0.2)  # 80/20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9dab380-43c7-47c3-9e79-93eac9b053a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f547ed7c43499a9f0af7f27d5efd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/586 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735d1132c1ec4eddb64d5532977b989f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization and Fine-Tuning\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoModel\n",
    "\n",
    "# Load the tokenizer\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Adjust num_labels to your task\n",
    "\n",
    "def str2int(str):\n",
    "    if str == \"nao-protesto\":\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized[\"label\"] = [str2int(label) for label in examples[\"label\"]]\n",
    "    return tokenized\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39626293-423d-446e-8b99-b81d34e76df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff5fe9c114c485faa94a1771268e610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b621b0659f84446b0142324f95065d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.12384030967950821, 'eval_runtime': 4.3164, 'eval_samples_per_second': 34.056, 'eval_steps_per_second': 4.402, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c924bb42e2b4160a49aab5041938825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06446218490600586, 'eval_runtime': 4.2732, 'eval_samples_per_second': 34.401, 'eval_steps_per_second': 4.446, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f98ace6bbc44ed8205f6658bae3388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05415595695376396, 'eval_runtime': 4.3695, 'eval_samples_per_second': 33.642, 'eval_steps_per_second': 4.348, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17c472dfcf1467ea80a28a115041e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07869672775268555, 'eval_runtime': 4.3838, 'eval_samples_per_second': 33.532, 'eval_steps_per_second': 4.334, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec0672ea5d1b492a9765017476cfe28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0805230438709259, 'eval_runtime': 4.4196, 'eval_samples_per_second': 33.261, 'eval_steps_per_second': 4.299, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8275097698fb4426894227e76fe19f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08206836879253387, 'eval_runtime': 4.4616, 'eval_samples_per_second': 32.948, 'eval_steps_per_second': 4.259, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f572ad33ed64357ba5f9229e4612616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08430156111717224, 'eval_runtime': 4.4234, 'eval_samples_per_second': 33.232, 'eval_steps_per_second': 4.295, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e799705e1e4f909381b0a95bd0e11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08617407828569412, 'eval_runtime': 4.3753, 'eval_samples_per_second': 33.597, 'eval_steps_per_second': 4.343, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7f4e97b320e47f2a82c3969ddc8b337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.086964912712574, 'eval_runtime': 3.6346, 'eval_samples_per_second': 40.445, 'eval_steps_per_second': 5.228, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09cbfcebb0974badabe0b606160d9a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08806697279214859, 'eval_runtime': 3.7066, 'eval_samples_per_second': 39.659, 'eval_steps_per_second': 5.126, 'epoch': 10.0}\n",
      "{'train_runtime': 522.3269, 'train_samples_per_second': 11.219, 'train_steps_per_second': 0.708, 'train_loss': 0.03017582248997044, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=370, training_loss=0.03017582248997044, metrics={'train_runtime': 522.3269, 'train_samples_per_second': 11.219, 'train_steps_per_second': 0.708, 'total_flos': 1541830784409600.0, 'train_loss': 0.03017582248997044, 'epoch': 10.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b964e041-b4a0-4efa-a234-5c22007474fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_model\\\\tokenizer_config.json',\n",
       " './trained_model\\\\special_tokens_map.json',\n",
       " './trained_model\\\\vocab.txt',\n",
       " './trained_model\\\\added_tokens.json',\n",
       " './trained_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save both the model and tokenizer\n",
    "trainer.save_model(\"./trained_model\")  # Saves the model\n",
    "tokenizer.save_pretrained(\"./trained_model\")  # Saves the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6f3062-e348-4cd4-b23b-20f8a38bedaa",
   "metadata": {},
   "source": [
    "# Now let's run inference on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4836df5d-8221-410f-b3c6-f149eaa43799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Path to your saved model\n",
    "model_path = \"./trained_model\"\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) \n",
    "vocabDict = tokenizer.get_vocab()\n",
    "\n",
    "def getClass(pred):\n",
    "    if pred == 0:\n",
    "        return 'nao-protesto'\n",
    "    return 'protesto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeefbe22-3b03-4220-be65-85f883d5fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exampleText = \"Um homem, suspeito de ter aplicado um golpe em um idoso de Paranaguá, teve o celular e dispositivos eletrônicos apreendidos nesta quinta-feira (17). O crime aconteceu em abril deste ano, quando o suspeito fingiu que era o filho da vítima e o convenceu a depositar todo o salário na conta bancária do criminoso. De acordo com as investigações do Gaeco de Paranaguá, o idoso teve prejuízos financeiros e abalos psicológicos. A apreensão dos itens do suspeito foi feita pelo Ministério Público no município de Aparecida, em Goiás.De acordo com as investigações, o suspeito criava perfis falsos no WhatsApp para pedir dinheiro. O delegado que atua no Gaeco, Fernando de Carvalho Santana explica que casos de estelionato como esse são comuns e por isso as pessoas devem prestar mais atenção.A Polícia Civil do Paraná tem uma cartilha com dicas para orientar a população sobre esses golpes aplicados por meio de mensagens, para que as pessoas aprendam a reconhecer e a evitar a ação de estelionatários. Acesse aqui a cartilha da Polícia Civil.Reportagem: Brenda Niewiorowski\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(exampleText, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9972e662-2112-45e4-aca2-79002df9e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: protesto\n",
      "Probabilities: tensor([[0.4120, 0.5880]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Run inference with the trained model\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Get the model's logits and apply softmax to get probabilities\n",
    "logits = outputs.logits\n",
    "probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Get the predicted class\n",
    "predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "\n",
    "# Output the result\n",
    "print(f\"Predicted class: {getClass(predicted_class)}\")\n",
    "print(f\"Probabilities: {probabilities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be1c7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
